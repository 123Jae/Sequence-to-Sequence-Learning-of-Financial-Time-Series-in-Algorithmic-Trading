Time series forecasting---predicting future values of variables within a
domain---is a largely unsolved problem in complex or chaotic domains such as
weather (e.g. humidity, temperature or wind speed) and economics (e.g. currency
exchange rates or stock prices).  Examining the problem with the latest models
in the field of machine learning---\textsc{lstm}-based \textsc{rnn}s---gives
rise to new hope of deepening our understanding of the problem.

\section{Background and motivation}
Artificial neural networks (\textsc{ann}s) have been applied for several decades
to solve classification (determining what \textit{class} a data point belongs
to) and regression (determining the \textit{value} of a dependent variable as a
function of a data point's coordinates) problems.  The early versions of
\textsc{ann}s could only solve classification problems \citep{rosenblatt1958},
while more recent \textsc{ann} models are also capable of handling regression
problems (CITATION PLZ).  An \textsc{ann}, expressed in simple terms, is a
virtual model of a biological brain, consisting of neurons, synapses and
dendrites, modelled by the \textsc{ann} through cells with scalars, activation
functions and weights.

The earlier models were also unable to predict time series sequences (i.e.
determining what class a data point belongs to with respect to its movement over
time, or determining the value of a dependent variable as a function of the data
point's movement over time), something that was later solved through the
introduction of \textsc{rnn}s \citep*{rumelhart1986}, which can handle datasets with
points that have a temporal component dictating their intradependence and
distribution within the dataset, where each point in the sequence is correlated
to previous (with respect to time) points through several known and unknown
factors.

\textsc{rnn}s have historically had problems with vanishing (or exploding)
gradients (i.e. the \textsc{rnn}s break down during training and become unable
to learn) \citep{pascanu2012}, but this problem was solved through the
introduction of the \textsc{lstm}-based \textsc{rnn}s \citep*{hochreiter1997}.
More recently, deep \textsc{lstm}-based \textsc{rnn}s have been applied with 
some level of success in predicting future time series sequences from 
historical, sequential datasets; such as predicting the weather for the next 
twenty-four hours \citep*{zaytar2016}, or predicting the next sequence of words 
in natural language \citep*{quoc2014}.

Regardless of whether the problem is approached as a matter of classification or
regression, the temporal aspect renders algorithms not specifically designed for
the task impotent since one single input dataset could belong to several
different classes depending on where, in time or order, it appears in the data
sequence.  The problem should therefore be examined with algorithms designed
specifically for time series sequences, such as \textsc{lstm}-based
\textsc{rnn}s.

\section{Current research}
Currently, in the field of machine learning, hidden Markov models
(\textsc{hmm}s), dynamic Bayesian networks (\textsc{dbn}s), tapped-delay neural
networks (\textsc{tdnn}s) and \textsc{rnn}s are commonly used to handle
sequential datasets, and have been applied with some degree of success to
financial markets \citep{saad1998,kita2012,zhang2004}.

We believe, however, that \textsc{lstm}-based \textsc{rnn}s can be applied to
the problem with greater success.

\section{Problem statement}
Financial time series are used ubiquitously in algorithmic trading.  In
algorithmic trading, it is imperative that accurate predictions are made about
numerous variables (such as volatility and returns) in order to time market
entry and exit.

Since deep \textsc{lstm}-based \textsc{rnn}s have only been applied within the
algorithmic trading domain to a minimal extent, and since they have shown great
success in solving similar problems in other domains, it raises the question
whether the technique can be used to predict a future sequence of financial
variables that can be used to time both entry and exit positions within a
certain time horizon.

Presuming that correlations exist along the temporal dimension of the dataset,
the problem is reduced to a matter of finding an appropriate set of
\textit{features} enhancing the correlations, on which to train the
\textsc{lstm}-based \textsc{rnn}.  Expressed in a more concise manner, we
attempt to answer the question:

Can financial markets be predicted accurately through the application of
\textsc{lstm}-based \textsc{rnn}s?
